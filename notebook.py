# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B6sTo5tCmL4ep7b68JwrTKIWm78xjdAQ
"""

# Mengimpor library yang dibutuhkan
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import RandomOverSampler, SMOTE
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

"""### EDA

Tahapan ini melibatkan proses evaluasi kualitas data, pembersihan, transformasi, serta eksplorasi awal untuk mengidentifikasi pola dan karakteristik yang relevan dalam dataset.
"""

# Load dataset
df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')
# Menmpilkan 10 data terakhir
df.tail(10)

# Memeriksa struktur data
df.info()

# Mengubah tipe data kolom TotalCharges dari object ke float
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Mengubah tipe data kolom SeniorCitizen menjadi object karna merupakan fitur kategorikal
df["SeniorCitizen"] = df["SeniorCitizen"].astype("O")

# Mengubah repsentasi variabel Churn dari 'Yes' dan 'No' menjadi 1 dan 0
df["Churn"] = df["Churn"].apply(lambda x : 1 if x == "Yes" else 0)

df.head()

# Mengecek missing value
df.isnull().sum()

# Menghapus missng value yang ada di kolom TotalCharges
df.dropna(subset=['TotalCharges'], inplace=True)

df.info()

"""Pada tahapan ini, kita memisahkan kolom kategorikal dan numerik untuk mempermudah proses transformasi data.  """

# Mengelompokkan fitur kategorikal
cat_cols = [col for col in df.columns if df[col].dtypes == "O" and col != 'customerID']
print(cat_cols)

# Mengelompokkan fitur numerik
num_cols = [col for col in df.columns if df[col].dtypes != "O" and col != 'Churn']
print(num_cols)

"""#### Univariate Analysis

Melakukan visualisasi terhadap fitur kategorikal dan numerik sekaligus untuk menganalisis hubungan masing-masing fitur dengan status churn.
Grafik di bawah ini menyajikan distribusi serta persentase pelanggan yang churn dan tidak churn pada setiap kategori atau rentang nilai fitur.

Analisis Kolom Kategorikal
"""

def cat_summary(dataframe, col_name, target=None, plot=False):
    """Menampilkan rasio setiap kategori dan (opsional) distribusi target seperti 'Churn'"""
    if target:
        percent_table = pd.crosstab(dataframe[col_name], dataframe[target], normalize='index') * 100
        print(f"Persentase distribusi {target} pada '{col_name}':")
        print(percent_table.round(2))
    else:
        print(pd.DataFrame({
            col_name: dataframe[col_name].value_counts(),
            "Ratio": 100 * dataframe[col_name].value_counts() / len(dataframe)
        }))
    print("--------------------------------------")

    if plot:
        if target:
            sns.countplot(x=col_name, data=dataframe, hue=target)
            plt.title(f"Distribusi {col_name} berdasarkan {target}")
        else:
            sns.countplot(x=col_name, data=dataframe)
            plt.title(f"Countplot of {col_name}")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

# Menampilkan ratio dan visualisasi dari kolom kategorikal
for col in cat_cols:
    cat_summary(df, col, target='Churn', plot=True)

"""Analisis Kolom Numerik"""

def plot_numeric_distributions(df, num_cols):
    """Menampilkan  visualisasi histogram dari kolom numerik"""
    for col in num_cols:
        plt.figure(figsize=(8, 4))
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Frequency')
        plt.tight_layout()
        plt.show()

plot_numeric_distributions(df, num_cols)

# Menampilkan parameter statistik kolom numerik
df.describe()

"""Analisis Korelasi"""

# Menampilkan korelasi dari kolom numerik
corr = df[num_cols].corr()
corr

# Visualisasi matrix korelasi
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(numeric_only=True), annot=False, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

"""Heatmap korelasi digunakan untuk melihat hubungan antar fitur numerik. Fitur seperti tenure, monthly charges, dan total charges menunjukkan korelasi tertentu terhadap churn.

#### Multivariate Analysis
"""

# Visualisasi Churn dengan tenure
sns.boxplot(x='Churn', y='tenure', data=df)

"""**Insight**:
- pelanggan yang churn (1) rata-rata punya tenure yang jauh lebih pendek, banyak yang kurang dari 10 bulan.
- ada beberapa pelanggan churn tapi tenure-nya panjang (outlier)
- distribusi pelanggan yang tidak churn (0) lebih panjang dan stabil
"""

# Visualisasi tenure dengan TotalCharges
sns.scatterplot(data=df, x='tenure', y='TotalCharges', hue='Churn', alpha=0.7)
plt.title("Scatter Plot of Tenure vs TotalCharges")
plt.xlabel("Tenure (months)")
plt.ylabel("Total Charges")
plt.show()

"""**Insight**:
- tenure dan totalcharges punya hubungan positif. pola naik yang cukup jelas
- churn cenderung lebih banyak di titik dimana semakin tinggi tenure semakin naik totalcharges.
- pelanggan den tenure pendek dan total charges kecil adalah segmen paling rentan churn
- ada beberapa pelanggan yg tenure nya lama tapi bayarnya tetep kecil. mungkin faktor diskon atau ga banyak make service.
"""

# Visualisasi distribusi churn dengan Contract
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='Contract', hue='Churn', palette='viridis')
plt.title('Distribusi Churn berdasarkan Tipe Kontrak')
plt.xlabel('Tipe Kontrak')
plt.ylabel('Jumlah Pelanggan')
plt.legend(title='Churn')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

# Melihat persentase pelanggan churn berdasarkan Tipe Contract
churn_percent = pd.crosstab(df['Contract'], df['Churn'], normalize='index') * 100
churn_percent = churn_percent.rename(columns={'No': 'Not Churned (%)', 'Yes': 'Churned (%)'})

print(churn_percent)

"""**Insight**:
- pelanggan churn banyak dari karyawan month-to-month daripada tahunan
"""

# Visualisasi distribusi churn dengan InternetService
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='InternetService', hue='Churn', palette='viridis')
plt.title('Distribusi Churn berdasarkan Tipe Internet Service')
plt.xlabel('Tipe Internet Service')
plt.ylabel('Jumlah Pelanggan')
plt.legend(title='Churn')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

"""**Insight**:
- pelanggan yang pake fiber optic paling banyak churn.
"""

# Visualisasi distribusi churn dengan PaymentMethod
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='PaymentMethod', hue='Churn', palette='viridis')
plt.title('Distribusi Churn berdasarkan Tipe Pembayaran')
plt.xlabel('Tipe Pembayaran')
plt.ylabel('Jumlah Pelanggan')
plt.legend(title='Churn')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

# Persentase Churn berdasarkan PaymentMethod
pd.crosstab(df['PaymentMethod'], df['Churn'], normalize='index') * 100

"""**Insight**:
- pelanggan yang membayar dengan electronic check lebih banyak churn.
"""

# Visualisasi Churn berdasarkan PaymentMethod dan Contract
g = sns.catplot(data=df, x='PaymentMethod', hue='Churn', col='Contract', kind='count', palette='Set2')
g.fig.suptitle('Distribusi Churn berdasarkan PaymentMethod dan Contract', y=1.05)
g.set_axis_labels("Tipe Pembayaran", "Jumlah Pelanggan")
g._legend.set_title('Churn')

# Rotasi label x-axis
for ax in g.axes.flatten():
    ax.set_xticklabels(ax.get_xticklabels(), rotation=15)

plt.show()

"""**Insight**:
- karyawan month-to-month banyak yg pake electronic check daripada karyawan tahunan. karyawan tahunan lebih keliatan loyal keliatan dari mereka bayar pake credit card atau pemabayran otomatis

### Modeling

Sebelum melakukan modeling, kita perlu melakukan data preparation untuk memastikan data siap digunakan oleh model machine learning. Tahapan yang dilakukan meliputi melakukan encoding untuk kolom kategorikal, standarisasi untuk kolom numerik, melakukan resampling jika data tidak seimbang, memisahkan fitur dan target.
"""

# Memisahkan kolom Churn dari cat_cols yang akan di encoding
cat_cols = [col for col in cat_cols if col not in ['Churn']]
cat_cols

# One-hot encoding untuk fitur kategorikal
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

df.head()

# Standarisasi pada kolom  numerik
scaler = RobustScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

df.head()

# Memisahkan kolom Churn dan CustomerID dari fitur (x) dengan kolom target (y)
X = df.drop(['Churn', 'customerID'], axis=1)
y = df['Churn']

# Melihat jumlah data pada kolom target
y.value_counts()

"""Kita bisa melihat distribusi kelas pelanggan churn dan tidak churn ternyata tidak seimbang. Maka kita perlu melakukan oversampling pada kelas minoritas yaitu kelas churn. SMOTE digunakan untuk resampling karna SMOTE membuat data sintetis alih-alih menduplikat data yang sudah ada."""

# Melakukan resampling dengan SMOTE
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Membagi data train dan test dari hasil resampling
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

"""Data dibagi menjadi data train dan test dengan proporsi 80:20 agar model dapat dievaluasi terhadap data yang belum pernah dilihatnya."""

# Mengecek kembali jumlah data
y_resampled.value_counts()

"""**Model Logistic Regression**

Membangun model logistic regression dengan parameter default di awal kemudian mencoba menemukan kombinasi parameter yang bagus untuk meningkatkan performa model menggunakan grid search. Dapat dilihat di bawah perbandingan metrik klasifikasi yang dihasilkan.
"""

# Model 1: Logistic Regression
lr = LogisticRegression(max_iter=1000, class_weight='balanced')
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Logistic Regression:\n", classification_report(y_test, y_pred_lr))

param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['liblinear', 'saga'],
    'class_weight': ['balanced', None],
    'max_iter': [500, 1000]
}

grid_search = GridSearchCV(LogisticRegression(), param_grid,
                           cv=5, scoring='f1', n_jobs=-1, verbose=1)

grid_search.fit(X_train, y_train)

# Hasil terbaik
print("Best Parameters:", grid_search.best_params_)
best_lr = grid_search.best_estimator_

# Evaluasi di test set
y_pred_best = best_lr.predict(X_test)
print("Best Logistic Regression Model:\n", classification_report(y_test, y_pred_best))

# Confusion matrix Logistic Regression
cm_lr = confusion_matrix(y_test, y_pred_best)

plt.figure(figsize=(5,4))
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=["Not Churn", "Churn"], yticklabels=["Not Churn", "Churn"])
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Mari kita lihat fitur-fitur yang berpengaruh terhadap churn dan tidak churn pada model logistic regression menggunakan coefficients. Semakin besar nilainya, semakin berpengaruh fitur tersebut terhadap churn dan tidak churn.
Koefisien menggambarkan seberapa besar kontribusi suatu fitur dalam membantu model membedakan antara pelanggan yang Churn dan yang Tidak Churn.
"""

coefs = pd.DataFrame({
    "Feature": X.columns,
    "Coefficient": best_lr.coef_[0]
}).sort_values(by="Coefficient", ascending=False)

# Menampilkan fitur yang cenderung mendorong ke churn (positif koefisien)
coefs[coefs["Coefficient"] > 0]

"""Kita bisa mengambil kesimpulan dengan melihat kembali grafik bar univariate analysis di atas untuk menganalisis lebih lanjut. Kesimpulan sementara:
- pelanggan yang menggunakan fiber optic sebagai layanan internet memiliki persentase tinggi untuk pelanggan churn.
- pelanggan lebih banyak yang menggunakan phoneservice daripada yang tidak
- banyak pelanggan yang menggunakan  layanan streaming.

**Model Random Forest**
"""

# Model 2: Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest:\n", classification_report(y_test, y_pred_rf))

"""Mencari kombinasi parameter terbaik model random forest untuk meningkatkan performa menggunakan grid search."""

# Parameter grid untuk dicoba
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2']
}



# GridSearch dengan 5-fold cross-validation
grid_search = GridSearchCV(estimator=rf,
                           param_grid=param_grid,
                           cv=5,
                           n_jobs=-1,
                           scoring='f1',
                           verbose=1)

# Fit model ke data training
grid_search.fit(X_train, y_train)

# Tampilkan hasil terbaik
print("Best Parameters:", grid_search.best_params_)
print("Best Score (f1):", grid_search.best_score_)

# Gunakan model terbaik
best_rf = grid_search.best_estimator_
y_pred_best_rf = best_rf.predict(X_test)

# Evaluasi hasilnya
from sklearn.metrics import classification_report
print("Optimized Random Forest:\n", classification_report(y_test, y_pred_best_rf))

# Confusion matrix Random Forest
cm_rf = confusion_matrix(y_test, y_pred_best_rf)

plt.figure(figsize=(5,4))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=["Not Churn", "Churn"], yticklabels=["Not Churn", "Churn"])
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Tahapan selanjutnya adalah melihat fitur-fitur yang berpengaruh untuk model random forest menggunakan feature_importances_."""

def plot_importance(model, features, num=len(X), save=False):
    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})
    plt.figure(figsize=(10, 10))
    sns.set(font_scale=1)
    sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value",
                                                                      ascending=False)[0:num])
    plt.title(f'Feature Importance - {model.__class__.__name__}')
    plt.tight_layout()
    plt.show(block=True)
    if save:
        plt.savefig(f'importances_{model.__class__.__name__}.png')

plot_importance(best_rf, X)

"""Fitur-fitur yang paling berkontribusi besar terhadap random forest adalah fitur numerik. Berbeda dengan logistic regression, fitur kategorikal justru memberikan kontribusi lebih besar terhadap model untuk membedakan pelanggan churn dan tidak churn. Ini karena logistic regression menggunakan pola linear, dan fitur kategorikal sudah kita ubah menggunakan one-hot encoding, lalu menghitung koefisien masing-masing fitur. Sedangkan random forest menggunakan gini impurity untuk menentukan fitur yang paling berkontribusi terhadap model.

Fitur-fitur yang paling berkontribusi besar pada model Random Forest cenderung berasal dari fitur numerik. Sebaliknya, pada model Logistic Regression, fitur kategorikal justru memberikan kontribusi lebih besar dalam membedakan pelanggan yang churn dan tidak churn.

Hal ini terjadi karena Logistic Regression merupakan model linear yang sangat bergantung pada representasi fitur. Fitur kategorikal telah diubah menjadi representasi numerik melalui one-hot encoding, dan model menghitung koefisien (weights) untuk masing-masing fitur, yang mencerminkan seberapa besar pengaruhnya terhadap probabilitas churn.

Sementara itu, Random Forest menentukan pentingnya fitur berdasarkan seberapa besar fitur tersebut mengurangi impurity (misalnya Gini impurity) saat membagi node dalam decision tree. Proses ini secara alami lebih peka terhadap fitur numerik yang memiliki variasi kontinu, karena dapat membagi data lebih fleksibel dibanding fitur kategorikal biner hasil one-hot.

Model random forest dipilih karna bagus dalam menangani fitur kategorikal dan numerik, mampu menangkap interaksi antar fitur dan pola non-linearitas, dan lebih robust terhadap outlier. Model ini juga memiliki recall yang tinggi. Karena kita ingin memprediksi pelanggan yang berpotensi churn. Karena dalam konteks prediksi churn, false negative (pelanggan yang akan churn tapi diprediksi tidak churn) bisa merugikan bisnis. Maka dari itu, kita ingin meminimalkan kesalahan ini dengan mengoptimalkan recall — agar sebanyak mungkin pelanggan yang berpotensi churn bisa terdeteksi dan ditindaklanjuti.
"""